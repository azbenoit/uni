{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Examples\n",
    "\n",
    "The codes below illustrate various algorithms introduced in the course: gradient descent with fixed step, with variable step, with optimal step, etc. \n",
    "\n",
    "Experiment with the code by changing the objective function, the number of iteration, the tolerance, the step size, etc. Try to understand how the visualization is done using the contour function. \n",
    "\n",
    "You may use parts of this code as a starting point for the case where a line-search procedure is used instead of a fixed descent step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import *\n",
    "%matplotlib notebook\n",
    "\n",
    "A = np.array([[1.0,0],[0,2]])\n",
    "print(A)\n",
    "\n",
    "ev = np.linalg.eig(A)[0]\n",
    "print(\"Eigenvalues of A: \",ev)\n",
    "\n",
    "maxstep = 2/np.max(ev)\n",
    "optstep = 2/(np.max(ev)+np.min(ev))\n",
    "\n",
    "print(\"Maximal step for GD: \",maxstep)\n",
    "print(\"Optimal step for GD: \",optstep)\n",
    "\n",
    "b = np.array([1,1])\n",
    "\n",
    "x0 = np.array([-0.5,0])\n",
    "\n",
    "def J(x):\n",
    "    return 0.5*np.dot(x,A@x)-np.dot(b,x)\n",
    "def GradJ(x):\n",
    "    return A@x-b\n",
    "\n",
    "print(\"Initial value:\",J(x0))\n",
    "\n",
    "Tol = 1e-6\n",
    "plt.figure(dpi=150)\n",
    "xmin=-1\n",
    "xmax=2\n",
    "ymin=-1\n",
    "ymax=2\n",
    "aX0=np.linspace(xmin,xmax)\n",
    "aX1=np.linspace(ymin,ymax)\n",
    "Z=np.array([[J(np.array([x0,x1])) for x0 in aX0] for x1 in aX1])\n",
    "plt.contour(aX0,aX1,Z,12)\n",
    "plt.plot(x0[0],x0[1],'rx')\n",
    "plt.axis('scaled')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$x_0$')\n",
    "plt.ylabel('$x_1$')\n",
    "plt.title('Isovalues of $f(x,y)$ and initial point $x_{init}$')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientFixedStep(f,df,x_init,step=1e-01,tol=1e-06,maxiter=200):\n",
    "    # initialization\n",
    "    x=x_init.copy()\n",
    "    xtab=[]\n",
    "    ftab=[]\n",
    "    xtab.append(x) # add x to the list xtab\n",
    "    ftab.append(f(x))\n",
    "    it=0 # iteration count\n",
    "    while((it==0) or (it<maxiter and np.linalg.norm(xtab[-1]-xtab[-2]))>tol):\n",
    "        it=it+1\n",
    "        g = df(x)\n",
    "        # line-search \n",
    "        x=x-step*g\n",
    "        xtab.append(x)\n",
    "        ftab.append(f(x))\n",
    "    # boolean to indicate the convergence\n",
    "    if(it==maxiter):\n",
    "        conv = False\n",
    "    else:\n",
    "        conv = True\n",
    "    return xtab, ftab, conv\n",
    "xtab,ftab,conv = gradientFixedStep(J,GradJ,x0)\n",
    "print('Has the algorithm converged ? : ',conv)\n",
    "plt.figure()\n",
    "plt.plot(ftab)\n",
    "plt.xlabel('iteration: $it$')\n",
    "plt.ylabel('$J(it)$')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "aX0=np.linspace(xmin,xmax)\n",
    "aX1=np.linspace(ymin,ymax)\n",
    "Z=np.array([[J(np.array([x0,x1])) for x0 in aX0] for x1 in aX1])\n",
    "plt.contour(aX0,aX1,Z,25)\n",
    "\n",
    "lx0=[X[0] for X in xtab]\n",
    "lx1=[X[1] for X in xtab]\n",
    "plt.plot(lx0,lx1,\"-ro\")\n",
    "plt.title('Number of iterations. = '+str(np.shape(lx0)[0]))\n",
    "plt.axis('scaled')\n",
    "plt.colorbar()\n",
    "#plt.savefig('FixedStep001.png',dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try maxstep, optstep\n",
    "xtab2,ftab2,conv = gradientFixedStep(J,GradJ,x0,step=1e-3,maxiter=2000)\n",
    "print('Has the algorithm converged ? : ',conv)\n",
    "plt.figure()\n",
    "plt.plot(ftab2)\n",
    "plt.xlabel('iteration: $it$')\n",
    "plt.ylabel('$J(it)$')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "aX0=np.linspace(xmin,xmax)\n",
    "aX1=np.linspace(ymin,ymax)\n",
    "Z=np.array([[J(np.array([x0,x1])) for x0 in aX0] for x1 in aX1])\n",
    "plt.contour(aX0,aX1,Z,12)\n",
    "\n",
    "lx0=[X[0] for X in xtab2]\n",
    "lx1=[X[1] for X in xtab2]\n",
    "plt.plot(lx0,lx1,\"-ro\")\n",
    "\n",
    "plt.axis('scaled')\n",
    "plt.title('nb iter. = '+str(np.shape(lx0)[0]))\n",
    "plt.colorbar()\n",
    "#plt.savefig('FixedStep00001.png',dpi=300)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "lx0=x0[0]\n",
    "lx1=x0[1]\n",
    "line, = ax.plot(lx0,lx1,\"-ro\")\n",
    "aX0=np.linspace(-3,3)\n",
    "aX1=np.linspace(-3,3)\n",
    "Z=np.array([[J(np.array([x0,x1])) for x0 in aX0] for x1 in aX1])\n",
    "CF=ax.contourf(aX0,aX1,Z,20)\n",
    "CB=ax.contour(aX0,aX1,Z,20)\n",
    "plt.colorbar(CF)\n",
    "fig.suptitle('')\n",
    "\n",
    "def update(w):\n",
    "    xtabm,ftabm,conv = gradientFixedStep(J,GradJ,x0,step=w,maxiter=2000)\n",
    "    lx0=[X[0] for X in xtabm]\n",
    "    lx1=[X[1] for X in xtabm]\n",
    "    line.set_xdata(lx0)\n",
    "    line.set_ydata(lx1)\n",
    "    ax.set_title('nb iter. = '+str(np.shape(lx0)[0])+'; convergence ? : '+str(conv))\n",
    "    fig.canvas.draw()\n",
    "\n",
    "interact(update,w=widgets.FloatSlider(min=0.001, max=maxstep, step=.001, value=0.1, description='pas du grad'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientVariableStep(f,df,x_init,step=0.1,tol=1e-06,maxiter=200):\n",
    "    # initialization\n",
    "    maxstep = 10\n",
    "    x=x_init.copy()\n",
    "    xtab=[]\n",
    "    ftab=[]\n",
    "    xtab.append(x) # add x to the list xtab\n",
    "    pval = f(x)\n",
    "    ftab.append(pval)\n",
    "    it=0 # iteration count\n",
    "    g=df(x)\n",
    "    while((it==0) or (step>tol and it<maxiter and np.linalg.norm(xtab[-1]-xtab[-2]))>tol):  \n",
    "        #linesearch... this gives you the step\n",
    "        actx= x-step*g\n",
    "        val = f(actx)\n",
    "        if(val<pval):\n",
    "            #accept iteration\n",
    "            x = actx\n",
    "            pval = val\n",
    "            step = min(1.1*step,maxstep)\n",
    "            xtab.append(x)\n",
    "            ftab.append(val)\n",
    "            g = df(x)\n",
    "            it=it+1\n",
    "        else:\n",
    "            #refuse iteration\n",
    "            step = 0.8*step\n",
    "            \n",
    "    # boolean to indicate the convergence\n",
    "    if(it==maxiter):\n",
    "        conv = False\n",
    "    else:\n",
    "        conv = True\n",
    "    return xtab, ftab, conv\n",
    "xtab3,ftab3,conv = gradientVariableStep(J,GradJ,x0,step=0.1,maxiter=2000)\n",
    "print('Has the algorithm converged ? : ',conv)\n",
    "plt.figure()\n",
    "plt.plot(ftab3)\n",
    "plt.xlabel('iteration: $it$')\n",
    "plt.ylabel('$J(it)$')\n",
    "plt.show()\n",
    "plt.figure()\n",
    "aX0=np.linspace(xmin,xmax)\n",
    "aX1=np.linspace(ymin,ymax)\n",
    "Z=np.array([[J(np.array([x0,x1])) for x0 in aX0] for x1 in aX1])\n",
    "plt.contour(aX0,aX1,Z,12)\n",
    "\n",
    "lx0=[X[0] for X in xtab3]\n",
    "lx1=[X[1] for X in xtab3]\n",
    "plt.plot(lx0,lx1,\"-ro\")\n",
    "\n",
    "plt.axis('scaled')\n",
    "plt.title('Number of iterations. = '+str(np.shape(lx0)[0]))\n",
    "plt.colorbar()\n",
    "#plt.savefig('VariableStep.png',dpi=300)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientOptimalStep(f,df,A,x0,tol=1e-08,maxiter=200):\n",
    "    # initialization\n",
    "    x=x0.copy()\n",
    "    xtab=[]\n",
    "    ftab=[]\n",
    "    xtab.append(x) # add x to the list of points\n",
    "    ftab.append(f(x))\n",
    "    it=0 # iteration\n",
    "    while((it==0) or (it<maxiter and np.abs(f(xtab[-1])-f(xtab[-2]))>tol)):\n",
    "        g = df(x)\n",
    "        opt_step = np.dot(g,g)/np.dot(g,A@g)\n",
    "        x=x-opt_step*g\n",
    "        xtab.append(x)\n",
    "        ftab.append(f(x))\n",
    "        it=it+1\n",
    "    # booléen pour indiquer la convergence\n",
    "    if(it==maxiter):\n",
    "        conv = False\n",
    "    else:\n",
    "        conv = True\n",
    "    return xtab, ftab, conv\n",
    "xtab4,ftab4,conv = gradientOptimalStep(J,GradJ,A,x0,Tol,maxiter=2000)\n",
    "print('Has the algorithm converged ? : ',conv)\n",
    "print(xtab4[-1])\n",
    "plt.figure()\n",
    "plt.plot(ftab4)\n",
    "plt.xlabel('iteration: $it$')\n",
    "plt.ylabel('$J(it)$')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "aX0=np.linspace(xmin,xmax)\n",
    "aX1=np.linspace(ymin,ymax)\n",
    "Z=np.array([[J(np.array([x0,x1])) for x0 in aX0] for x1 in aX1])\n",
    "plt.contour(aX0,aX1,Z,12)\n",
    "\n",
    "lx0=[X[0] for X in xtab4]\n",
    "lx1=[X[1] for X in xtab4]\n",
    "plt.plot(lx0,lx1,\"-ro\")\n",
    "\n",
    "plt.axis('scaled')\n",
    "plt.title('Number of iterations. = '+str(np.shape(lx0)[0]))\n",
    "plt.colorbar()\n",
    "#plt.savefig('VariableStep.png',dpi=300)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConjugateGradient(f,df,A,x0,tol=1e-08,maxiter=200):\n",
    "    # initialization\n",
    "    x=x0.copy()\n",
    "    xtab=[]\n",
    "    ftab=[]\n",
    "    xtab.append(x) # add x to the list of points\n",
    "    ftab.append(f(x))\n",
    "    it=0 # iteration\n",
    "    g = df(x)\n",
    "    d = -g\n",
    "    n = A.shape[0]\n",
    "    print(n)\n",
    "    while((it==0) or (it<maxiter and np.abs(f(xtab[-1])-f(xtab[-2]))>tol)):\n",
    "        opt_step = np.dot(df(x),df(x))/(np.dot(df(x),A@df(x)))\n",
    "        x=x-opt_step*df(x)\n",
    "        xtab.append(x)\n",
    "        ftab.append(f(x))\n",
    "        it=it+1\n",
    "    # booléen pour indiquer la convergence\n",
    "    if(it==maxiter):\n",
    "        conv = False\n",
    "    else:\n",
    "        conv = True\n",
    "    return xtab, ftab, conv\n",
    "\n",
    "xtab5,ftab4,conv = gradientOptimalStep(J,GradJ,A,x0,Tol,maxiter=2000)\n",
    "print('Has the algorithm converged ? : ',conv)\n",
    "print(xtab4[-1])\n",
    "plt.figure()\n",
    "plt.plot(ftab4)\n",
    "plt.xlabel('iteration: $it$')\n",
    "plt.ylabel('$J(it)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
